# logfile path
log-file: /var/log/deepflow/server.log
# loglevel: "debug/info/warn/error"
log-level: info

## open pprof serves via HTTP server port 9526. ref: https://pkg.go.dev/net/http/pprof
#profiler: false

## maximum usage of cpu cores, 0 means no limit
#max-cpus: 0

#continuous-profile:
#  enabled: false
#  server-addr: http://deepflow-agent/api/v1/profile
#  # support profile types: "cpu", "inuse_objects", "alloc_objects", "inuse_space", "alloc_space", "goroutines", "mutex_count", "mutex_duration", "block_count", "block_duration"
#  profile-types: ["cpu", "inuse_objects", "alloc_objects", "inuse_space", "alloc_space"]
#  mutex-rate: 5 # valid when ProfileTypes contains 'mutex_count' or 'mutex_duration'
#  block-rate: 5 # valid when ProfileTypes contains 'block_count' or 'block_duration'
#  log-enabled: true # whether record profile debug logs

## extract an integer (generally used timestamp) from traceId as an additional index to speed up traceId queries.
#trace-id-with-index:
#  enabled: false
#  type: hash           # hash/incremental-id
#  #eg. traceId: 'abcdef1234',  if want to match index '1234', 'format' must set 'decimal', `start` can be set as 6 or -4 , and `length` must set: 4
#  incremental-id-location:      # it is valid when 'type' is 'incremental-id'
#    start: 0           # >= 0 means counting from the beginning, < 0 means counting from the end.
#    length: 13         # when 'format' is 'decimal' 'length' range is (0, 20], 'format' is 'hex' 'length' range is (0, 16].
#    format: decimal    # hex/decimal

## monitor the disk usage of the paths
#monitor-paths: [/,/mnt,/var/log]

controller:
  ## controller http listenport
  #listen-port: 20417
  #listen-node-port: 30417
  ## grpc server port
  #grpc-port: 20035
  #ssl-grpc-port: 20135
  #agent_ssl_cert_file: /etc/ssl/server.key
  #agent_ssl_key_file: /etc/ssl/server.pem
  #ingester-port: 20033
  #grpc-node-port: 30035
  # grpc max message lenth default 100M
  grpc-max-message-length: 104857600
  # kubeconfig
  kubeconfig:
  # election
  election-name: deepflow-server
  # Once every 24 hours DeepFlow will report usage data to usage.deepflow.yunshan.net
  # The data includes a random ID, version, number of deepflow server and agent.
  # No data from user databases is ever transmitted.
  # Change this option to true to disable reporting.
  reporting-disabled: false
  # Deepflow billing mode  license/voucher
  billing-method: license

  # ingester plaform data, default: 0
  # 0 (All K8s Cluster)
  # 1 (K8s Cluster in local Region)
  # 2 (K8s Cluster in local AZs)
  pod-cluster-internal-ip-to-ingester: 0

  http:
    # resource api redis cache refresh interval, unit:s
    redis_refresh_interval: 3600
    # additional domains
    additional_domains:

  # deepflow web service config
  df-web-service:
    enabled: false
    host: df-web
    port: 20825
    timeout: 30

  # deepflow fpermit service config
  fpermit:
    enabled: false
    host: fpermit
    port: 20823
    timeout: 30

  # mysql相关配置
  mysql:
    database: deepflow
    user-name: root
    user-password: deepflow
    host: mysql
    port: 30130
    timeout: 30
    # whether drop database when init failed
    drop-database-enabled: false
    auto_increment_increment: 1
    # limit the total number of process queried at a time
    result_set_max: 100000

  # redis相关配置
  redis:
    enabled: true
    cluster_enabled: false
    resource_api_database: 1
    resource_api_expire_interval: 3600
    dimension_resource_database: 2
    password: deepflow
    host:
      - redis
    port: 6379
    timeout: 30

  # clickhouse相关配置
  clickhouse:
    database: flow_tag
    user-name: default
    host: clickhouse
    port: 9000
    # user-password:

  # datasource-api from ingester
  ingester-api:
    port: 30106
    timeout: 60

  # 规格相关定义
  spec:
    vtap_group_max: 1000
    vtap_max_per_group: 10000
    az_max_per_server: 10
    data_source_max: 25
    data_source_retention_time_max: 24000
    # unit: s
    data_source_ext_metrics_interval: 10
    # unit: s
    data_source_prometheus_interval: 10

  # monitor module config
  monitor:
    # controller/analyzer health_check interval, unit:s
    health_check_interval: 60
    # 健康检查异常/控制器切换处理channel的长度
    health_check_handle_channel_len: 1000
    # License检查的时间间隔，单位: 秒
    license_check_interval: 60
    # vtap检查的时间间隔，单位: 秒
    vtap_check_interval: 60
    # exception_time_frame, unit:s
    exception_time_frame: 3600
    # vtap rebalance config, interval uint:s
    auto_rebalance_vtap: true
    rebalance_check_interval: 300
    ingester-load-balancing-strategy:
      # options: by-ingested-data, by-agent-count
      algorithm: by-ingested-data 
      # use the data in data-duration as the basis for balancing, default: 1d, uint: s
      data-duration: 86400
      # rebalance vtap interval, default: 1h, uint: s
      rebalance-interval: 3600
    # automatically delete lost vtaps, uint:s
    vtap_auto_delete_interval: 3600
    # warrant
    warrant:
      host: warrant
      port: 20413
      timeout: 30

  # manager module config
  manager:
    # 云平台增加/删除/配置变化检测的时间间隔，单位：秒
    cloud_config_check_interval: 60
    task:
      # recorder更新数据库的时间间隔，单位：秒
      resource_recorder_interval: 60
      cloud:
        # Kubernetes数据获取的时间间隔，单位：秒
        kubernetes_gather_interval: 30
        # 阿里公有云API获取区域列表时，需要指定一个区域
        aliyun_region_name: cn-beijing
        # AWS API获取区域列表时，需要指定一个区域，并通过这个区域区分国际版和国内版
        aws_region_name: cn-north-1
        # 采集器同步默认vpc名称
        genesis_default_vpc: default_vpc
        # 配置宿主机IP文件
        hostname_to_ip_file: /etc/hostname_to_ip.csv
        # 开启debug模式，支持debug云平台API/SDK原始数据
        debug_enabled: false
        # 云平台同步api调用超时时间，单位：秒
        http_timeout: 30
        # custom tag value 最大长度限制
        custom_tag_len_max: 256
        # process name 最大长度限制
        process_name_len_max: 256
      recorder:
        # recorder模块缓存自愈刷新时间间隔，单位：分钟
        cache_refresh_interval: 60
        # 软删除资源数据清理时间间隔，单位：小时
        # 此值应小于 soft_deleted_resource_expire_interval
        deleted_resource_clean_interval: 24
        # 软删除资源数据保留时间，单位：小时，默认：7 * 24
        deleted_resource_retention_time: 168
        # 资源ID限制：区域、可用区、宿主机、VPC、网络、容器集群、命名空间
        resource_max_id_0: 64000
        # 资源ID限制：所有设备ID（除宿主机外）、容器节点、Ingress、工作负载、ReplicaSet、POD
        resource_max_id_1: 499999
        # local debug log
        log_debug: 
          enabled: false
          detail_enabled: false
          resource_type:
          #  - all
          #  - vpc
  tagrecorder:
    # size of data in batch operation for MySQL
    mysql_batch_size: 1000
    live_view_refresh_second: 60

  trisolaris:
    tsdb_ip:
    chrony:
      host: 127.0.0.1
      port: 123
      timeout: 1

    trident-type-for-unkonw-vtap: 0

    platform-vips:
    #  - 55.11.135.18

    # master/slave 区域标识, 默认为主区域，部署时会自动修改
    node-type: master

    # 区域服务域名前缀
    region-domain-prefix: ""

    # 采集器是否自动注册
    vtap-auto-register: True

    default-tap-mode:

    # whether to register domain automatically
    domain-auto-register: True

    # clean up the data and cache of the table kubernetes_cluster according to the data 
    # that was not synchronized before a certain period of time 
    clear-kubernetes-time: 600

  genesis:
    # 平台数据老化时间，单位：秒
    aging_time: 86400
    # 采集器接口数据老化时间，单位：秒
    vinterface_aging_time: 300
    # 无子网IP的最大掩码长度
    ipv4_cidr_max_mask: 24
    ipv6_cidr_max_mask: 64

    # 采集器上报数据处理的队列长度
    queue_length: 1000

    # 数据持久化检测间隔，单位：秒
    data_persistence_interval: 60

    # 采集器同步KVM时，配置的采集器IP所上报的内容会被解析
    host_ips:
    # - x.x.x.x
    # - x.x.x.x/x
    # - x.x.x.x-x.x.x.x

    # 内网IP网段范围配置，配置的IP所在网络会被作为内网进行学习
    local_ip_ranges:
    # - x.x.x.x/x
    # - x.x.x.x-x.x.x.x

    # 排除IP范围
    exclude_ip_ranges: 
    # - x.x.x.x/x
    # - x.x.x.x-x.x.x.x

    # 多namespace模式开关，默认false
    multi_ns_mode:
    # 单独vpc模式开关，默认false
    single_vpc_mode:
    # 忽略网卡正则表达式配置，匹配到会忽略该网卡，默认为 ^(kube-ipvs) ，增加其他的网卡名称需要在此基础上新增
    ignore_nic_regex:

  prometheus:
    # synchronizer cache refresh interval, unit: second
    synchronizer_cache_refresh_interval: 60
    # encoder cache refresh interval, unit: second
    encoder_cache_refresh_interval: 3600
    # time interval for regularly clearing prometheus expired data, unit: minute, default: 60 * 24
    # time interval should be greater than or equal to ingester: prometheus-label-cache-expiration configuration
    data_clean_interval: 1440

querier:
  # querier http listenport
  listen-port: 20416
  language: en

  # clickhouse相关配置
  clickhouse:
    database: flow_tag
    user-name: default
    host: clickhouse
    port: 9000
    timeout: 60
    max-connection: 20
    # user-password:

  # profile相关配置
  profile:
    flame_query_limit: 1000000

  # deepflow-app相关配置
  deepflow-app:
    host: deepflow-app
    port: 20418

  otel-endpoint: http://deepflow-agent/api/v1/otel/trace
  limit: 10000
  time-fill-limit: 20

  prometheus:
    limit: 1000000
    qps-limit: 100 # setting to 0 means no limit
    series-limit: 500
    max-samples: 50000000
    auto-tagging-prefix: df_
    request-query-with-debug: true
    external-tag-cache-size: 1024
    external-tag-load-interval: 300
    thanos-replica-labels: [] # remove duplicate replica labels when query data
    cache:
      remote-read-cache: true
      response-cache: false
      cache-item-size: 512000 # max size of cache item, unit: byte
      cache-max-count: 1024 # max capacity of cache list
      cache-first-timeout: 10 # time out for first cache item load, uint: s
      cache-clean-interval: 3600 # clean interval for cache, unit: s
      cache-allow-time-gap: 1 # when query end - cache end < gap, not update cache, unit: s

  auto-custom-tag:
    tag-name: 
    tag-values: 
  # external-apm:
  # - name: skywalking
  #   addr: 127.0.0.1:12800

ingester:
  ## whether Ingester store metrics/flow_log... to database
  #storage-disabled: false

  #ckdb:
  #  # use internal or external ckdb
  #  external: false
  #  host: deepflow-clickhouse
  #  port: 9000
  #  # for get clickchouse endpoints tcp port value
  #  endpoint-tcp-port-name: tcp-port
  #  # if `external` is 'true', default value is 'default', else 'df_cluster'
  #  cluster-name:
  #  # if `external` is 'true', default value 'default', else 'df_storage'
  #  storage-policy:
  #  A list of supported time zones can be found in https://www.iana.org/time-zones and also can be queried by SELECT * FROM system.time_zones
  #  time-zone: Asia/Shanghai

  ## This configuration is only valid when the ClickHouse tables have not yet been created. If the tables have been created, you need to delete the databases/tables and restart to create the tables according to the configuration
  #ckdb-cold-storage:
  #  enabled: false
  #  cold-disk: # have configured in clickhouse
  #    type: volume  # 'volume' or 'disk'
  #    name: xxx
  #  settings:
  #  - db: flow_log
  #    # if 'tables' is empty, will set all tables under the DB
  #    tables:
  #    - l4_flow_log
  #    - l7_flow_log
  #    # uint: hour, move data to cold disk after 'ttl-hour-to-move'
  #    ttl-hour-to-move: 24
  #  - db: flow_metrics
  #    tables:
  #    - vtap_flow_port.1m
  #    - vtap_flow_edge_port.1m
  #    ttl-hour-to-move: 168

  #ckdb-auth:
  #  username: default
  #  # '#','@' special characters are not supported in passwords
  #  password:

  # local node ip, if not set will get from environment variable 'NODE_IP', dafault: ""
  #node-ip:

  ## trisolaris的ips, 默认值为空
  #controller-ips:
  #  - x.x.x.x

  ## controller listening port
  #controller-port: 20035

  ## es的相关的配置syslog写es
  #es-host-port:
  #  - 127.0.0.1:20042

  #es-auth:
  #  user:
  #  password:

  ## stats collect interval(unit: s)
  # stats-interval: 10

  ## The listening port used by Ingester to receive data
  #listen-port: 20033

  ## 遥测数据写入配置
  #metrics-ck-writer:
  #  queue-count: 1      # 每个表并行写数量
  #  queue-size: 1000000 # 数据队列长度
  #  batch-size: 512000  # 多少行数据同时写入
  #  flush-timeout: 10   # 超时写入时间

  ## 流日志写入配置
  #flowlog-ck-writer:
  #  queue-count: 1      # 每个表并行写数量
  #  queue-size: 1000000 # 数据队列长度
  #  batch-size: 512000  # 多少行数据同时写入
  #  flush-timeout: 10   # 超时写入时间

  ## ext metrics写入配置
  #ext-metrics-ck-writer:
  #  queue-count: 1      # 每个表并行写数量
  #  queue-size: 100000  # 数据队列长度
  #  batch-size: 51200   # 多少行数据同时写入
  #  flush-timeout: 10   # 超时写入时间

  ## ext_metrics database data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #ext-metrics-ttl-hour: 168

  ## flow_metrics database data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #flow-metrics-ttl-hour:
  #  vtap-flow-1m: 168     # vtap_flow[_edge]_port.1m
  #  vtap-flow-1s: 24     # vtap_flow[_edge]_port.1s
  #  vtap-app-1m: 168      # vtap_app[_edge]_port.1m
  #  vtap-app-1s: 24      # vtap_app[_edge]_port.1s

  ## flow_metrics database data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #flow-log-ttl-hour:
  #  l4-flow-log: 72
  #  l7-flow-log: 72
  #  l4-packet: 72

  ## event data write config
  #event-ck-writer:
  #  queue-count: 1      # 每个表并行写数量
  #  queue-size: 50000  # 数据队列长度
  #  batch-size: 25600   # 多少行数据同时写入
  #  flush-timeout: 5   # 超时写入时间

  ## resource event table data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #event-ttl-hour: 720

  ## alarm event table data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #alarm-event-ttl-hour: 720

  ## perf event data write config
  #perf-event-ck-writer:
  #  queue-count: 1      # 每个表并行写数量
  #  queue-size: 50000  # 数据队列长度
  #  batch-size: 25600   # 多少行数据同时写入
  #  flush-timeout: 5    # 超时写入时间

  ## perf event table data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #perf-event-ttl-hour: 168

  ## pcap data write config
  #pcap-ck-writer:
  #  queue-count: 1     # 每个表并行写数量
  #  queue-size: 50000  # 数据队列长度
  #  batch-size: 2048   # 多少行数据同时写入
  #  flush-timeout: 5   # 超时写入时间

  ## pcap database data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #pcap-ttl-hour: 72

  ## pcap decoder queue count/size
  #pcap-queue-count: 2
  #pcap-queue-size: 10000

  ## profile process data write config
  #profile-ck-writer:
  #  queue-count: 1      # parallelism of table writing
  #  queue-size: 100000  # size of writing queue
  #  batch-size: 51200   # size of batch writing
  #  flush-timeout: 5    # timeout of table writing

  ## profile process database data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #profile-ttl-hour: 72

  ## profile compression algorithm, default is zstd, empty string for not compress
  #profile-compression-algorithm: "zstd"

  ## 默认读超时，修改数据保留时长时使用
  #ck-read-timeout: 300

  ## prometheus data writer config
  #prometheus-ck-writer:
  #  queue-count: 1      # parallelism of table writing
  #  queue-size: 524288  # size of writing queue
  #  batch-size: 262144  # size of batch writing
  #  flush-timeout: 10   # timeout of table writing

  ## prometheus decoder queue count/size
  #prometheus-decoder-queue-count: 1
  #prometheus-decoder-queue-size: 16384

  ## prometheus database data retention time(unit: hour)
  ## Note: This configuration is only valid when DeepFlow is run for the first time or the ClickHouse tables have not yet been created
  #prometheus-ttl-hour: 168

  ## prometheus label request/response msg max size (unit: bytes)
  #prometheus-label-msg-max-size: 104857600

  ## when prometheus requests labels, how many metric batch requests
  #prometheus-label-request-metric-batch-count: 128
  #prometheus-app-label-column-increment: 4
  #prometheus-app-label-column-min-count: 8

  ## Whether to ignore the writing of Universal Tag, the default is false, which means writing
  #prometheus-sample-ignore-universal-tag: false

  ## prometheus cache expiration of label ids. uint: s
  #prometheus-label-cache-expiration: 86400

  #ck-disk-monitor:
  #  check-interval: 300 # 检查时间间隔(单位: 秒)
  ## 磁盘空间不足时，同时满足磁盘占用率>used-percent和磁盘空闲<free-space, 或磁盘占用大于used-space, 开始清理数据
  ## When the disk space is insufficient, the disk occupancy > 'used-percent' and the disk idle < 'free-space' are met at the same time, or the disk occupancy > 'used-space', then the data is cleaned up
  #  disk-cleanups:
  #  - disk-name-prefix: path_  # monitor disks starting with 'disk-name-prefix', check the disks 'select * from system.disks'
  #    used-percent: 80         #  disk usage threshold, ranges: 0-100
  #    free-space: 100          #  uint: GB, disk minimum free threshold
  #    used-space: 0            #  uint: GB, disk maximum usage space threshold. (If it is 0, it means ignore the condition)
  #  priority-drops:     # set which database and table data will be deleted first when disk is full
  #  - database: flow_log
  #    tables-contain:      # tables name containing the string will be priority-dropped. If it is empty, it means all the tables under the database
  #  - database: flow_metrics
  #    tables-contain: 1s_local

  ## ingester模块是否启用，默认启用, 若不启用(表示处于单独的控制器)
  #ingester-enabled: true

  # whether to enable deepflow-aget syslog to write to files in syslog-directory
  #agent-log-to-file: false

  ## syslog存储位置
  #syslog-directory: /var/log/deepflow-agent

  ## syslog是否写入elasticsearch，默认启用
  #es-syslog: true

  ## udp socket receiver buffer: 64M
  #udp-read-buffer: 67108864

  ## tcp socket receiver buffer: 4M
  #tcp-read-buffer: 4194304

  ## tcp socket reader buffer: 1M
  #tcp-reader-buffer: 1048576

  ## Rpc synchronization recv/send msg buffer(unit: Byte)
  #grpc-buffer-size: 41943040

  ## query platformData service, port filter fastmap LRU capacity(unit: count)
  #service-labber-lru-cap: 4194304

  ## ########################### flow metrics config #############################################

  ## 是否不写入秒级数据: 默认: false(写入)
  #disable-second-write: false

  ## parallelism of unmarshall, defaults to 4
  #unmarshall-queue-count: 4

  ## size of unmarshall queue, defaults to 10240
  #unmarshall-queue-size: 10240

  ## the maximum threshold for processing l4/l7 flow logs per second.(threshold for each flow log). If set to 0, the threshold for processing is not limited
  #throttle: 50000
  ## Sampling bucket count. The larger this value is, the more accurate the sampling current limit is, and the more memory it takes up.
  #throttle-bucket: 8

  ## 分别控制l4流日志, l7流日志，默认值为0，表示使用throttle的设置值.若非0，则使用设置值
  #l4-throttle: 0
  #l7-throttle: 0

  #flow-log-decoder-queue-count: 2
  #flow-log-decoder-queue-size: 10000

  #ext-metrics-decoder-queue-count: 2
  #ext-metrics-decoder-queue-size: 10000

  #profile-decoder-queue-count: 2
  #profile-decoder-queue-size: 10000

  #event-decoder-queue-count: 1
  #event-decoder-queue-size: 10000

  #perf-event-decoder-queue-count: 2
  #perf-event-decoder-queue-size: 100000

  ## unit: byte
  #flow-tag-cache-max-size: 262144

  ## unit: s
  #flow-tag-cache-flush-timeout: 1800

  #exporters:
  #  enabled: false
  #  # export datas ranges: cbpf-net-span, ebpf-sys-span
  #  export-datas: [cbpf-net-span,ebpf-sys-span]
  #  # export-data-types ranges: service_info,tracing_info,network_layer,flow_info,client_universal_tag,server_universal_tag,tunnel_info,transport_layer,application_layer,capture_info,native_tag,metrics
  #  export-data-types: [service_info,tracing_info,network_layer,flow_info,transport_layer,application_layer,metrics]
  #  export-custom-k8s-labels-regexp:  # type string, default: "", means not export custom k8s labels. for example: ".+", means export all custom k8s labels; "aaa|bbb": means export labels which contains 'aaa' or 'bbb' string. ref: https://github.com/google/re2/wiki/Syntax
  #  export-only-with-traceid: false # if set 'true', if the span has no trace id, it will not be exported
  #  otlp-exporters:
  #  - enabled: false
  #    addr: 127.0.0.1:4317 # grpc protobuf addr, only support protocol 'grpc'
  #    queue-count: 4       # parallelism of sender
  #    queue-size: 100000   # size of each exporter queue
  #    export-datas: [cbpf-net-span,ebpf-sys-span]
  #    export-data-types: [service_info,tracing_info,network_layer,flow_info,transport_layer,application_layer,metrics]
  #    export-custom-k8s-labels-regexp:
  #    export-only-with-traceid: false
  #    export-batch-count: 32 # ExportRequest contains defalut(32) ResourceSpans(each l7_flow_log corresponds to a ResourceSpans)
  #    grpc-headers: # grpc headers, type: map[string]string, default is null, the following is an example configuration
  #      key1: value1
  #      key2: value2

  #metrics-prom-writer:
  #  enabled: false
  #  endpoint:               #eg. http://1.2.3.4:9091
  #  headers:                # type: map[string]string, extra http request headers
  #  batch-size: 2048        # each http request contians how many timeseries
  #  flush-timeout: 5
  #  queue-count: 2
  #  queue-size:  1000000
  #  metrics-filter: [app] # only support: 'app', means flow_metrics.'vtap_app_edgp_port.1s'/'vtap_app_port.1s'
