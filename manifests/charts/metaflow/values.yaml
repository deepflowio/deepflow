# Default values for metaflow-server.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
global:
  image:
    ## ghcr Image repository address: ghcr.io/metaflowys
    ## Dockerhub Image repository address: metaflowys
    ## AliyunYun Image repository address: registry.cn-beijing.aliyuncs.com/metaflowys
    repository: metaflowys
    pullPolicy: Always
  hostNetwork: false
  dnsPolicy: ClusterFirst
  password:
    mysql: metaflow
    #grafana: metaflow 
  podManagementPolicy: "OrderedReady"
  replicas: 1  ## replicas for metaflow-server and clickhouse
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []
  podAffinityLabelSelector: []
  podAffinityTermLabelSelector: []
  nodeAffinityLabelSelector: []
  nodeAffinityTermLabelSelector: []
  timezone: "Asia/Shanghai"
  nodePort: 
    clickhouse: 30900
    metaflowServerIngester: 30033
    metaflowServerGrpc: 30035
    metaflowServerSslGrpc: 30135
    metaflowServerhealthCheck: 30417
  ntpServer: ntp.aliyun.com
  ## Whether to enable allInone local storage, if enabled, the local /opt directory is used to store data by default, ignoring the node affinity check, and is not responsible for any data persistence
  allInOneLocalStorage: false 
  storageClass: ""

image:
  server:
    repository: "{{ .Values.global.image.repository }}/metaflow-server"
    tag: latest
    pullPolicy: "{{ .Values.global.image.pullPolicy }}"
  elector:
    repository: "{{ .Values.global.image.repository }}/elector"
    tag: latest
    pullPolicy: "{{ .Values.global.image.pullPolicy }}"
  app:
    repository:  "{{ .Values.global.image.repository }}/metaflow-app"
    tag: latest
    pullPolicy: "{{ .Values.global.image.pullPolicy }}"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

timezone: "{{ .Values.global.timezone }}"
podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

server:
  replicas: "{{ .Values.global.replicas }}"
  hostNetwork:  "{{ .Values.global.hostNetwork }}"
  dnsPolicy: "{{ .Values.global.dnsPolicy }}"
  podManagementPolicy: "{{ .Values.global.podManagementPolicy }}"
  service:
    ## Configuration for Metaflow-server service
    ##
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Metaflow-server Service to listen on
    ##

    ports:
    - name: querier
      port: 20416
      targetPort: 20416
      nodePort:
      protocol: TCP
    - name: health-check
      port: 20417
      targetPort: 20417
      nodePort: "{{ .Values.global.nodePort.metaflowServerhealthCheck }}"
      protocol: TCP
    - name: grpc
      port: 20035
      targetPort: 20035
      nodePort:
      protocol: TCP
    - name: grpc-30035
      port: 30035
      targetPort: 20035
      nodePort: "{{ .Values.global.nodePort.metaflowServerGrpc }}"
      protocol: TCP
    - name: grpc-30035-udp
      port: 30035
      targetPort: 20035
      nodePort: "{{ .Values.global.nodePort.metaflowServerGrpc }}"
      protocol: UDP
    - name: ssl-grpc
      port: 20135
      targetPort: 20135
      nodePort: "{{ .Values.global.nodePort.metaflowServerSslGrpc }}"
      protocol: TCP
    - name: ingester
      port: 20033
      targetPort: 20033
      nodePort:
      protocol: TCP
    - name: ingester-30033
      port: 30033
      targetPort: 20033
      nodePort: "{{ .Values.global.nodePort.metaflowServerIngester }}"
      protocol: TCP
    - name: ingester-30033-udp
      port: 30033
      targetPort: 20033
      nodePort: "{{ .Values.global.nodePort.metaflowServerIngester }}"
      protocol: UDP
    ## Additional ports to open for server service
    additionalPorts: []

    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    ## must be Local
    externalTrafficPolicy: Local

    ## Service type
    ##
    type: NodePort

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  podAntiAffinityLabelSelector:
    - labelSelector:
      - key: app
        operator: In
        values: metaflow
      - key: component
        operator: In
        values: metaflow-server
      topologyKey: "kubernetes.io/hostname"
  podAntiAffinityTermLabelSelector: []
  podAffinityLabelSelector:
    - labelSelector:
      - key: app
        operator: In
        values: metaflow
      - key: component
        operator: In
        values: clickhouse
      topologyKey: "kubernetes.io/hostname"
  podAffinityTermLabelSelector: []
  nodeAffinityLabelSelector: []
  nodeAffinityTermLabelSelector: []
  extraVolumeMounts: []
  #   - name: extra-volume-0
  #     mountPath: /mnt/volume0
  #     readOnly: true
  #     existingClaim: volume-claim
  #   - name: extra-volume-1
  #     mountPath: /mnt/volume1
  #     readOnly: true
  #     hostPath: /usr/shared/


app:
  replicas: "1"
  hostNetwork:  "{{ .Values.global.hostNetwork }}"
  dnsPolicy: "{{ .Values.global.dnsPolicy }}"
  service:
    ## Configuration for Metaflow querier service
    ##
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Metaflow querier service to listen on
    ##
    ports:
    - name: app
      port: 20418
      targetPort: 20418
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 
      protocol: TCP

    ## Additional ports to open for Metaflow querier service
    additionalPorts: []

    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    ##
    externalTrafficPolicy: Cluster

    ## Service type
    ##
    type: ClusterIP
  resources: {}
  nodeSelector: {}
  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []
  podAffinityLabelSelector: []
  podAffinityTermLabelSelector: []
  nodeAffinityLabelSelector: []
  nodeAffinityTermLabelSelector: []

tolerations: []



config:
  ## no blank lines
  ## 不要有空行
  server.yaml: |
    controller:
      ## logfile path
      log-file: /var/log/metaflow/server.log
      ## loglevel: "debug/info/warn/error"
      log-level: info
      listen-port: 20417
      ## mysql config
      mysql:
        database: deepflow
        user-name: root
        user-password: {{ .Values.global.password.mysql }}
        host: {{ $.Release.Name }}-mysql
        port: 30130
        timeout: 30
      ## clickhouse config
      clickhouse:
        database: flow_tag
        user: default
        port: {{ .Values.global.nodePort.clickhouse }}
        timeout: 60
        host: {{ $.Release.Name }}-clickhouse
      ## roze
      roze:
        port: 20106
        timeout: 60
      ## Specification related Definition
      spec:
        vtap_group_max: 1000
        vtap_max_per_group: 10000
        az_max_per_server: 10
        data_source_max: 10
        data_source_retention_time_max: 1000
      monitor:
        ## Controller and data node check Interval (unit: second)
        health_check_interval: 60
        ## Health check exception/Controller switchover process channel length
        health_check_handle_channel_len: 1000
        ## License check interval (unit: second)
        license_check_interval: 60
        ## warrant
        warrant:
          host: warrant
          port: 20413
          timeout: 30
      manager:
        ## Interval for cloud platform to add/delete/configure change detection (unit: second)
        cloud_config_check_interval: 60
        ## task module config
        task:
          ## Recorder Database update interval, unit: second
          resource_recorder_interval: 60
        cloud:
          ## Kubernetes Interval for data acquisition, in seconds
          kubernetes_gather_interval: 60
          ## When ali Public Cloud API obtains the region list, it needs to specify a region
          aliyun_region_name: cn-beijing
        recorder:
          ## Recorder module cache self-healing refresh interval, unit: minute
          cache_refresh_interval: 3600
      trisolaris:
        tsdb_ip:
        chrony:
          host: {{ tpl .Values.global.ntpServer . }}
          port: 123
          timeout: 1
        # - 127.0.0.1
        trident-port: 20035
        max-escape-seconds: 3600
        trident-type-for-unkonw-vtap: 0
        platform-vips:
        #  - 55.11.135.18
        node-type: "master"
        ## grpc max message lenth default 100M
        grpc-max-message-length: 104857600
        region-domain-prefix: ""
      genesis:
        aging_time: 86400
        vinterface_aging_time: 300
        grpc_server_port: {{ .Values.global.nodePort.metaflowServerGrpc }}
        exclude_ip_ranges: []
        queue_length: 1000
        data_persistence_interval: 60
    querier:
      # logfile path
      log-file: /var/log/metaflow/server.log
      # loglevel: "debug/info/warn/error"
      log-level: info
      # querier http listenport
      listen-port: 20416
      # clickhouse config
      clickhouse:
        database: flow_tag
        user: default
        port: 9000
        timeout: 60
        host: {{ $.Release.Name }}-clickhouse
    ingester:
      ckdb-service-prefix: {{ $.Release.Name }}-clickhouse-headless
  app.yaml: |
    app:
      # logfile path
      log-file: /var/log/metaflow/app.log
      # loglevel: "debug/info/warn/error"
      log-level: info
      # app http listenport
      listen-port: 20418
      # http request/response timeout
      http_request_timeout: 600
      http_response_timeout: 600
      querier:
        host: {{ include "metaflow.fullname" . }}-server
        port: 20416
        timeout: 60
      controller:
        host: {{ include "metaflow.fullname" . }}-server
        port: 20417
        timeout: 60
      spec:
        l7_tracing_limit: 100

clickhouse:
  # Default values for clickhouse.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  replicas: "{{ .Values.global.replicas }}"
  hostNetwork:  "{{ .Values.global.hostNetwork }}"
  dnsPolicy: "{{ .Values.global.dnsPolicy }}"
  podManagementPolicy: "{{ .Values.global.podManagementPolicy }}"
  image:
    repository: clickhouse/clickhouse-server
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "21.8.15.7"

  timezone: "{{ .Values.global.timezone }}"
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000
  storageConfig:
    ## persistentVolumeClaim/hostPath
    ## If you use hostPath, you must configure nodeAffinityLabelSelector, otherwise your data will be lost when Pod drifts, ignored allInOneLocalStorage=true 
    ## 如果使用hostPath存储clickhouse数据，则必须配置nodeAffinityLabelSelector，避免pod漂移导致数据丢失，allInOneLocalStorage=true 时忽略
    type: persistentVolumeClaim
    generateType: "{{ if $.Values.global.allInOneLocalStorage }}hostPath{{ else }}{{$.Values.storageConfig.type}}{{end}}" #Please ignore this
    hostPath: /opt/metaflow-clickhouse
    persistence:
      - name: clickhouse-path
        accessModes:
        - ReadWriteOnce
        size: 100Gi
        annotations: 
        storageClass: "{{ .Values.global.storageClass }}"
        # selector:
        #   matchLabels:
        #     app.kubernetes.io/name: clickhouse
      - name: clickhouse-storage-path
        accessModes:
        - ReadWriteOnce
        size: 200Gi
        annotations: 
        storageClass: "{{ .Values.global.storageClass }}"
        # selector:
        #   matchLabels:
        #     app.kubernetes.io/name: clickhouse
    s3StorageEnabled: false

  clickhouse:
    interserverHttpPort: 9009
    maxConcurrentQueries: 2000
    ## 单次查询最大内存 (bytes)
    maxMemoryUsage: 10000000000
    maxQuerySize: 10737418240
    maxAstElements: 2000000
    maxExpandedAstElements: 2000000
    connectTimeout: 500
    backgroudPoolSize: 32
  service:
    ## Configuration for Clickhouse service
    ##
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Clickhouse Service to listen on
    ##

    ports:
    - name: http-monitor-port
      port: 8123
      targetPort: 8123
      nodePort: 
      protocol: TCP
    - name: tcp-port
      port: 9000
      targetPort: 9000
      nodePort: "{{ .Values.global.nodePort.clickhouse }}"
      protocol: TCP
    - name: interserver-http-port
      port: 9009
      targetPort: 9009
      nodePort: 
      protocol: TCP

    ## Additional ports to open for server service
    additionalPorts: []

    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    ## must be Local
    externalTrafficPolicy: Local

    ## Service type
    ##
    type: NodePort


  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  podAntiAffinityLabelSelector:
    - labelSelector:
      - key: app
        operator: In
        values: metaflow
      - key: component
        operator: In
        values: clickhouse
      topologyKey: "kubernetes.io/hostname"
  podAntiAffinityTermLabelSelector: []
  podAffinityLabelSelector: []
  podAffinityTermLabelSelector: []
  nodeAffinityLabelSelector: []
    ## If you use hostPath, you must configure nodeAffinityLabelSelector, otherwise your data will be lost when Pod drifts
    ## 如果使用hostPath存储mysql数据，则必须配置nodeAffinityLabelSelector，避免pod漂移导致数据丢失
    # - matchExpressions:
    #     - key: kubernetes.io/hostname
    #       operator: In
    #       values: controller
  nodeAffinityTermLabelSelector: []

mysql:
  hostNetwork: "false"
  dnsPolicy: ClusterFirst
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  password: "{{ .Values.global.password.mysql }}"
  timezone: "{{ .Values.global.timezone }}"
  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext:
    ## If your mysql cannot start with hostPath, please open Privileged
    ## 如果你的mysql使用hostPath无法启动，请打开privileged
    # privileged: true
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: false
    # runAsNonRoot: false
    # runAsUser: 0
  storageConfig:
    ## persistentVolumeClaim/hostPath
    ## If you use hostPath, you must configure nodeAffinityLabelSelector, otherwise your data will be lost when Pod drifts
    ## 如果使用hostPath存储mysql数据，则必须配置nodeAffinityLabelSelector，避免pod漂移导致数据丢失
    type: persistentVolumeClaim
    generateType: "{{ if $.Values.global.allInOneLocalStorage }}hostPath{{ else }}{{$.Values.storageConfig.type}}{{end}}" #Please ignore this
    hostPath: /opt/metaflow-mysql
    hostPathChownContainerEnabled: true
    persistence:
      storageClass: "{{ .Values.global.storageClass }}"
      annotations:
        "helm.sh/resource-policy": keep
      # existingClaim: your-claim-pvc-name
      accessMode: ReadWriteOnce
      size: 50Gi 
      
  externalMySQL:
    enabled: false
    hostIP: 192.168.1.1
    port: 30130

  service:
    ## Configuration for Clickhouse service
    ##
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Clickhouse Service to listen on
    ##

    ports:
    - name: tcp
      port: 30130
      targetPort: 30130
      nodePort: 
      protocol: TCP
    ## Additional ports to open for server service
    additionalPorts: []

    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    ## must be Local
    externalTrafficPolicy: Cluster

    ## Service type
    ##
    type: ClusterIP


  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []
  podAffinityLabelSelector: []
  podAffinityTermLabelSelector: []
  nodeAffinityLabelSelector: []
    ## If you use hostPath, you must configure nodeAffinityLabelSelector, otherwise your data will be lost when Pod drifts
    ## 如果使用hostPath存储mysql数据，则必须配置nodeAffinityLabelSelector，避免pod漂移导致数据丢失
    # - matchExpressions:
    #     - key: kubernetes.io/hostname
    #       operator: In
    #       values: controller
  nodeAffinityTermLabelSelector: []


metaflow-agent:
  enabled: true
  image:
    repository: "{{ .Values.global.image.repository }}/metaflow-agent"
    pullPolicy: "{{ .Values.global.image.pullPolicy }}"
    # Overrides the image tag whose default is the chart appVersion.
    tag: latest

  imagePullSecrets: []
  nameOverride: ""
  agentFullnameOverride: "metaflow-agent"

  podAnnotations: {}
  nodeIPInjection: true
  podSecurityContext: {}
    # fsGroup: 2000
  hostNetwork: "true"
  securityContext: 
    privileged: true
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    ## Configuration for Clickhouse service
    ##
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Port for Clickhouse Service to listen on
    ##

    ports:
    - name: receive
      port: 80
      targetPort: receive
      nodePort: 
      protocol: TCP

    ## Additional ports to open for server service
    additionalPorts: []

    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    ## must be Local
    externalTrafficPolicy: Local

    ## Service type
    ##
    type: ClusterIP

  metaflowServerNodeIPS:
  - "{{ include "metaflow.fullname" . }}-server"
  kubernetesClusterId: 
  agentGroupId: 
  ## This command takes effect when agent-group-config external_agent_http_proxy_enabled=1 is configured using metaflow-ctl
  ## Set the port to the agent-group-config port
  externalAgentHttpProxyPort: 38086

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi


  nodeSelector: {}

  tolerations: []

  podAntiAffinityLabelSelector: []
  podAntiAffinityTermLabelSelector: []
  podAffinityLabelSelector: []
  podAffinityTermLabelSelector: []
  nodeAffinityLabelSelector: []
  nodeAffinityTermLabelSelector: []


grafana:
  enabled: true
  namespaceOverride: ""

  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
  ##
  forceDeployDatasources: false

  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
  ##
  forceDeployDashboards: false

  ## Deploy default dashboards
  ##
  defaultDashboardsEnabled: true

  ## Timezone for the default dashboards
  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
  ##
  defaultDashboardsTimezone: utc
  adminPassword: metaflow

  service:
    enabled: true
    type: NodePort

  rbac:
    create: true
  ## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)
  # useExistingRole: name-of-some-(cluster)role
    pspEnabled: false
    namespaced: true

  grafana.ini:
    paths:
      plugins: /var/lib/grafana/plugins
    analytics:
      check_for_updates: true
    log:
      mode: console
    database:
      type: mysql
      host: "{{ $.Release.Name }}-mysql:30130"
      name: grafana
      user: root
      password: "{{ .Values.global.password.mysql }}"
    plugins:
      allow_loading_unsigned_plugins: metaflow-querier-datasource,metaflow-apptracing-panel,metaflow-topo-panel
  sidecar:
    skipTlsVerify: true
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "metaflow-templates"

      ## Annotations for Grafana dashboard configmaps
      ##
      annotations: {}
      provider:
        folder: 'MetaFlow Templates'
        allowUiUpdates: false
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'metaflow-system'
        orgId: 1
        folder: 'MetaFlow System'
        type: file
        disableDeletion: false
        allowUiUpdates: false
        updateIntervalSeconds: 30
        options:
          foldersFromFilesStructure: false
          path: /tmp/metaflow-system-dashboards
  extraContainers: |
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_dashboard
      - name: LABEL_VALUE
        value: "metaflow-system"
      - name: FOLDER
        value: /tmp/metaflow-system-dashboards
      - name: RESOURCE
        value: both
      - name: SKIP_TLS_VERIFY
        value: "true"
      {{- if .Values.sidecar.image.sha }}
      image: "{{ .Values.sidecar.image.repository }}:{{ .Values.sidecar.image.tag }}@sha256:{{ .Values.sidecar.image.sha }}"
      {{- else }}
      image: "{{ .Values.sidecar.image.repository }}:{{ .Values.sidecar.image.tag }}"
      {{- end }}
      imagePullPolicy: {{ .Values.sidecar.imagePullPolicy }}
      name: metaflow-system-dashboard-providers
      volumeMounts:
      - mountPath: /tmp/metaflow-system-dashboards
        name: metaflow-system-dashboards
  extraInitContainers:
    - name: init-custom-plugins
      image: "{{ .Values.global.image.repository }}/metaflow-init-grafana:latest"
      imagePullPolicy: "{{ tpl .Values.global.image.pullPolicy . }}"
      command: ['sh']
      args:  
      - '-c'
      - "cp -raf /metaflow-plugins/* /var/lib/grafana/plugins/"
      volumeMounts:
      - mountPath: /var/lib/grafana/plugins/
        name: custom-plugins

  extraEmptyDirMounts: 
    - name: custom-plugins
      mountPath: /var/lib/grafana/plugins
    - name: metaflow-system-dashboards
      mountPath: /tmp/metaflow-system-dashboards

  datasources: 
   datasources.yaml:
      apiVersion: 1
      datasources:
      - name: MetaFlow
        type: metaflow-querier-datasource
        url: 
        access: proxy
        isDefault: true
        jsonData:
          requestUrl: http://{{ include "metaflow.fullname" . }}-server:20416
          traceUrl: http://{{ include "metaflow.fullname" . }}-app:20418
      - name: "MetaFlow ClickHouse"
        type: grafana-clickhouse-datasource
        jsonData:
          port: 9000
          server: "{{ $.Release.Name }}-clickhouse"
          username: default
        secureJsonData:
          password: ""
      - name: "MetaFlow MySQL"
        type: mysql
        url: "{{ $.Release.Name }}-mysql:30130"
        database: deepflow
        user: root
        password: metaflow
        jsonData:
          maxOpenConns: 5
          maxIdleConns: 5
          connMaxLifetime: 14400
        secureJsonData:
          password: "{{ .Values.global.password.mysql }}"

  ingress:
    enabled: false
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # Values can be templated
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    path: /

    # pathType is only for k8s >= 1.1=
    pathType: Prefix

    hosts:
      - chart-example.local
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation
    ## Or for k8s > 1.19
    # - path: /*
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation


    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local